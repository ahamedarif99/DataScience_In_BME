{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-0.23.0-cp37-cp37m-win_amd64.whl (6.8 MB)\n",
      "Requirement already up-to-date: scipy in c:\\users\\owner\\anaconda3\\envs\\tensorflow\\lib\\site-packages (1.4.1)\n",
      "Requirement already up-to-date: matplotlib in c:\\users\\owner\\anaconda3\\envs\\tensorflow\\lib\\site-packages (3.2.1)\n",
      "Requirement already up-to-date: pandas in c:\\users\\owner\\anaconda3\\envs\\tensorflow\\lib\\site-packages (1.0.3)\n",
      "Requirement already up-to-date: seaborn in c:\\users\\owner\\anaconda3\\envs\\tensorflow\\lib\\site-packages (0.10.1)\n",
      "Requirement already satisfied, skipping upgrade: joblib>=0.11 in c:\\users\\owner\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from scikit-learn) (0.14.1)\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-2.0.0-py3-none-any.whl (34 kB)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.13.3 in c:\\users\\owner\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from scikit-learn) (1.18.4)\n",
      "Requirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in c:\\users\\owner\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in c:\\users\\owner\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied, skipping upgrade: cycler>=0.10 in c:\\users\\owner\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil>=2.1 in c:\\users\\owner\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from matplotlib) (2.8.1)\n",
      "Requirement already satisfied, skipping upgrade: pytz>=2017.2 in c:\\users\\owner\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from pandas) (2020.1)\n",
      "Requirement already satisfied, skipping upgrade: six in c:\\users\\owner\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from cycler>=0.10->matplotlib) (1.14.0)\n",
      "Installing collected packages: threadpoolctl, scikit-learn\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 0.22.2.post1\n",
      "    Uninstalling scikit-learn-0.22.2.post1:\n",
      "      Successfully uninstalled scikit-learn-0.22.2.post1\n",
      "Successfully installed scikit-learn-0.23.0 threadpoolctl-2.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U scikit-learn scipy matplotlib pandas seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.keras\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, Dense, Flatten, MaxPooling2D\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "from matplotlib import figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1112, 10)\n",
      "(1112, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "X_train = X_train[:1112].reshape(1112,28,28,1)\n",
    "y_train = y_train[:1112].reshape(1112,1)\n",
    "\n",
    "X_test = X_test[:500].reshape(500,28,28,1)\n",
    "y_test = y_test[:500].reshape(500,1)\n",
    "\n",
    "X_train_norm = X_train / 255\n",
    "X_test_norm = X_test / 255\n",
    "y_train_catg= to_categorical(y_train)\n",
    "y_test_catg = to_categorical(y_test)\n",
    "print(y_train_catg.shape)\n",
    "print(X_train_norm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 27, 27, 16)        80        \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 13, 13, 16)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 6, 6, 16)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 576)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 20)                11540     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                210       \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 10)                110       \n",
      "=================================================================\n",
      "Total params: 11,940\n",
      "Trainable params: 11,940\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1000 samples, validate on 112 samples\n",
      "Epoch 1/7\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.9940 - accuracy: 0.2420 - val_loss: 1.6449 - val_accuracy: 0.3571\n",
      "Epoch 2/7\n",
      "1000/1000 [==============================] - 2s 2ms/sample - loss: 1.2063 - accuracy: 0.5980 - val_loss: 1.1142 - val_accuracy: 0.6071\n",
      "Epoch 3/7\n",
      "1000/1000 [==============================] - 2s 2ms/sample - loss: 0.7754 - accuracy: 0.7590 - val_loss: 1.0444 - val_accuracy: 0.6964\n",
      "Epoch 4/7\n",
      "1000/1000 [==============================] - 2s 2ms/sample - loss: 0.6322 - accuracy: 0.8130 - val_loss: 0.8904 - val_accuracy: 0.7232\n",
      "Epoch 5/7\n",
      "1000/1000 [==============================] - 2s 2ms/sample - loss: 0.5393 - accuracy: 0.8340 - val_loss: 0.8834 - val_accuracy: 0.7500\n",
      "Epoch 6/7\n",
      "1000/1000 [==============================] - 2s 2ms/sample - loss: 0.4927 - accuracy: 0.8490 - val_loss: 0.8182 - val_accuracy: 0.7411\n",
      "Epoch 7/7\n",
      "1000/1000 [==============================] - 2s 2ms/sample - loss: 0.4226 - accuracy: 0.8670 - val_loss: 0.8698 - val_accuracy: 0.7321\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x19d036dcec8>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn = Sequential()\n",
    "cnn.add(Conv2D(filters=16, kernel_size=(2,2), activation='relu', input_shape=(28, 28, 1)))\n",
    "cnn.add(MaxPooling2D(pool_size=(2,2)))\n",
    "cnn.add(MaxPooling2D(pool_size=(2,2)))\n",
    "cnn.add(Flatten())\n",
    "\n",
    "cnn.add(Dense(units=20, activation='relu'))\n",
    "cnn.add(Dense(units=10, activation='relu'))\n",
    "cnn.add(Dense(10, activation='sigmoid', name='Output'))\n",
    "cnn.summary()\n",
    "cnn.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "cnn.fit(X_train_norm, y_train_catg, epochs=7, batch_size=3, validation_split=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 0s 214us/sample - loss: 0.6579 - accuracy: 0.8160\n",
      "The accuracy for 1000 train samples with 500 test sample:  81.5999984741211\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = cnn.evaluate(X_test_norm, y_test_catg)\n",
    "predictions = cnn.predict(X_test_norm)\n",
    "print('The accuracy for 1000 train samples with 500 test sample: ', accuracy * 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "X_train_2 = X_train[:5556].reshape(5556,28,28,1)\n",
    "y_train_2 = y_train[:5556].reshape(5556,1)\n",
    "\n",
    "X_test_2 = X_test[:1000].reshape(1000,28,28,1)\n",
    "y_test_2 = y_test[:1000].reshape(1000,1)\n",
    "\n",
    "X_train_norm2 = X_train_2 / 255\n",
    "X_test_norm2 = X_test_2 / 255\n",
    "y_train_catg2 = to_categorical(y_train_2)\n",
    "y_test_catg2 = to_categorical(y_test_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 27, 27, 64)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 13, 13, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2304)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 20)                46100     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                210       \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 10)                110       \n",
      "=================================================================\n",
      "Total params: 46,740\n",
      "Trainable params: 46,740\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 5000 samples, validate on 556 samples\n",
      "Epoch 1/7\n",
      "5000/5000 [==============================] - 10s 2ms/sample - loss: 1.4085 - accuracy: 0.5526 - val_loss: 0.8455 - val_accuracy: 0.7842\n",
      "Epoch 2/7\n",
      "5000/5000 [==============================] - 8s 2ms/sample - loss: 0.5859 - accuracy: 0.8570 - val_loss: 0.5268 - val_accuracy: 0.8615\n",
      "Epoch 3/7\n",
      "5000/5000 [==============================] - 8s 2ms/sample - loss: 0.3698 - accuracy: 0.9080 - val_loss: 0.3965 - val_accuracy: 0.8921\n",
      "Epoch 4/7\n",
      "5000/5000 [==============================] - 8s 2ms/sample - loss: 0.2705 - accuracy: 0.9310 - val_loss: 0.3494 - val_accuracy: 0.8939\n",
      "Epoch 5/7\n",
      "5000/5000 [==============================] - 8s 2ms/sample - loss: 0.2107 - accuracy: 0.9466 - val_loss: 0.2967 - val_accuracy: 0.9209\n",
      "Epoch 6/7\n",
      "5000/5000 [==============================] - 8s 2ms/sample - loss: 0.1702 - accuracy: 0.9546 - val_loss: 0.2650 - val_accuracy: 0.9317\n",
      "Epoch 7/7\n",
      "5000/5000 [==============================] - 8s 2ms/sample - loss: 0.1403 - accuracy: 0.9628 - val_loss: 0.3024 - val_accuracy: 0.9191\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x19d027d8c48>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn = Sequential()\n",
    "cnn.add(Conv2D(filters=64, kernel_size=(2,2), activation='relu', input_shape=(28, 28, 1)))\n",
    "cnn.add(MaxPooling2D(pool_size=(2,2)))\n",
    "cnn.add(MaxPooling2D(pool_size=(2,2)))\n",
    "cnn.add(Flatten())\n",
    "cnn.add(Dense(units=20, activation='sigmoid'))\n",
    "cnn.add(Dense(units=10, activation='relu'))\n",
    "cnn.add(Dense(10, activation='sigmoid', name='Output'))\n",
    "cnn.summary()\n",
    "cnn.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "cnn.fit(X_train_norm2, y_train_catg2, epochs=7, batch_size=4, validation_split=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 1s 505us/sample - loss: 0.2555 - accuracy: 0.9210\n",
      "The accuracy for 5000 train samples with 1000 test sample:  92.10000038146973\n"
     ]
    }
   ],
   "source": [
    "loss2, accuracy2 = cnn.evaluate(X_test_norm2, y_test_catg2)\n",
    "predictions2 = cnn.predict(X_test_norm2)\n",
    "print('The accuracy for 5000 train samples with 1000 test sample: ', accuracy2 * 100)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the case that the model above does not have over 95% accurace, as sometimes it does and some times it doesnt, there are many ways to improve the model. Some of easiet ways is by one, we can decrease the amount of hidden layers, two we can decrease the number of features,  and three we can decrease the batch and kernal size. The biggest problem I have with this though is that this will  overfit  the model and in real world scenerio this will be a considered a bad  as we are trying to overfit the model to the desired accurace based on the test data we have.\n"
     ]
    }
   ],
   "source": [
    "print('In the case that the model above does not have over 95% accurace, as', \n",
    "      'sometimes it does and some times it doesnt, there are many ways to',\n",
    "      'improve the model. Some of easiet ways is by one, we can decrease the',\n",
    "      'amount of hidden layers, two we can decrease the number of',\n",
    "      'features,  and three we can decrease the batch and kernal size.',\n",
    "      'The biggest problem I have with this though is that this will  overfit',\n",
    "      ' the model and in real world scenerio this will be a considered a bad',\n",
    "      ' as we are trying to overfit the model to the desired accurace based on the test data we have.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
